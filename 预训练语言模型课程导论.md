# 预训练语言模型课程导论

### 目录

1.预训练
2.语言模型
3.词向量
4.word2vec
5.自然语言处理的预训练模型
6.RNN与LSTM
7.ELMo
8.transformer
9.GPT
10.Bert

### 1.预训练

(1)预训练有什么用？
(2)预训练是什么？
(3)与训练怎么用？

模型A：100000张分类鸭和鹅
模型B：100张分类猫和狗

(1)深度学习的项目：大数据支持
冻结：浅层参数不变
微调：浅层参数会跟着任务B训练而改变

(2)通过一个已经训练好的模型A去完成一个小数据量的任务B，
使用了模型A的浅层参数

(3)fairseq和transformers库
总结：一个任务A，一个任务B，两者极其相似，
任务A已经训练了一个模型A，
使用模型A的浅层参数去训练模型B。

## 统计语言模型

语言模型
语言(人说的话)+模型(表示某个东西，为了完成摸个任务)
1.P（“判断这个词的词性”），P（“判断这个词的磁性”）
2.“判断这个词的”

统计语言模型
用统计的方法去解决上述两个问题
"判断这个词的词性" = "判断"，"这个","词","的","词性"
用了一个条件概率的链式法则(概率论)

![image-20230226172420505](C:\Users\Jay Shen\AppData\Roaming\Typora\typora-user-images\image-20230226172420505.png)

解决第二个问题：

“判断这个词的____”

P(w_next | "判断","这个","词","的") (1)

词库(词典) V --> 新华字典，搞出一个集合，把所有词装到集合V里

把集合里的每一个词都进行上一步(1)的计算



词库V = {"词性"，"火星"}

P(词性| "判断","这个","词","的") 

P(火星| "判断","这个","词","的") 

P(词性| "判断","这个","词","的",...,".....") 
$$
P(W_(next)|判断，这个，词，的) = \frac{count(W_(next),判断，这个，词，的)}{count(判断，这个，词，的)}
$$

## n元统计语言模型

P(词性|"这个","词","的") 

P(火星|"这个","词","的") 

P(词性|"词","的") 

P(火星|"词","的") 

把n个词，取2个词(2元),取3个词(3元)

## 如何去计算？

词性是动词

判断单词的词性

磁性很强的磁铁

北京的词性是名词
$$
P(词性|的) = \frac{count(词性,的)}{count{的}}=\frac{2}{3}
$$

# 平滑策略

$$
P(策略|平滑) = \frac{0}{0}
$$

$$
P(w_i|w_(i-1))=\frac{count(w_(i-1),w_i)+1}{count(w_(i-1))+|V|}
$$

防止出现0和0的情况，|V|非常小

# 统计语言模型

语言模型：计算一句话的概率，计算下一个词可能是什么

统计语言模型：用统计的方法去解决语言模型的问题(条件概率)

a元语言模型：只取a个词(马尔科夫链)

平滑策略：如果要求的词不在文本库里面就很难受

