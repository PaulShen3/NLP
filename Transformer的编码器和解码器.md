## Transformer的编码器和解码器



## 问题1：为什么Decoder需要做Mask

为了解决训练阶段和测试阶段的gap(不匹配)

机器翻译：源语句(我爱中国)，目标语句(I love China)

训练阶段：解码器会有输入，这个输入是目标语句,通过已经生成的词去让解码器更好的生成。（每一次都会把所有信息告诉解码器）

测试阶段：解码器也会有输入，但此时测试的时候是不知道目标语句是什么的，这个时候，你每生成一个词，就会有多一个词放入目标语句中，每次生成的时候都是已经生成的词。（测试阶段只会把已经生成的词告诉解码器）

为了匹配，为了解决这个gap，masked self-attention就登场了，我在训练阶段，我就做一个masked，当你生成第一个词，我啥也不告诉你，当你生成了第二个词，我告诉你第一个词。

## 问题2：为什么Encoder给予decoders的是K,V矩阵

Q来源解码器，K=V来源编码器

Q是查询变量，Q是已经生成的词

K=V是源语句

当我们生成这个词的时候，通过已经生成的词和源语句做自注意力，就是确定源语句中哪些词对接下来的词的生成更有作用。

## Transformer的输入和输出

 